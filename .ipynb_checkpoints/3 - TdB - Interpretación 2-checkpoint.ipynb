{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretación del Teorema de Bayes 2\n",
    "\n",
    "## Estimación de parámetros de un modelo\n",
    "\n",
    "Una opción es no hacer ninguna suposición con respecto a la distribución a Priori de los parámetros del modelo y maximizar directamente el likelihood.\n",
    "A esta técnica se la conoce como MLE (Maximum Likelihood Estimation).\n",
    "\n",
    "Quiero hallar los parámetros $W$ de una distribución que hagan que la probabilidad de tener las observaciones a la salida sea máxima. Es decir, quiero maximizar:\n",
    "\n",
    "$$ P(X|W) $$\n",
    "\n",
    "donde:\n",
    "\n",
    "$$ X = [x_1, x_2, ..., x_N] $$\n",
    "\n",
    "X es un vector con las N observaciones.\n",
    "\n",
    "Si las observaciones son independientes, quiero maximizar:\n",
    "\n",
    "$$ P(x_1|W)P(x_2|W)...P(x_N|W) $$\n",
    "\n",
    "### Ejemplo: Estimar el parámetro p de una distribución de Bernoulli que haya sido ensayada N veces\n",
    "\n",
    "La probabilidad de haber tenido una salida con $ \\#1=k $ luego de haber ensayado la V.A N veces es:\n",
    "\n",
    "$$P(\\#1=k|p)=p^{k}(1-p)^{N-k}$$\n",
    "\n",
    "En nuestro caso $P(\\#1=k|p)$ es el likelihood. Si buscamos un estimador de máxima verosimilitud (MLE) la forma de hallarlo es hallando el valor de p que maximice $P(\\#1=k|p)$. Para ello podemos derivar P e igualar a cero. Como truco matemático en vez de derivar P, vamos a derivar el $\\log P$ e igualarlo a cero. Como el logaritmo es monótono creciente, la ubicación del máximo no cambia. A $\\log P$ se lo suele llamar Log Likelihood.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial \\log P(\\# 1=k|p)}{\\partial p}=\\frac{k}{p}-\\frac{N-k}{1-p}=0$$\n",
    "\n",
    "Despejando tenemos:\n",
    "\n",
    "$$ p=\\frac{k}{N} $$\n",
    "\n",
    "Es decir, el estimador del parámetro p que solemos utilizar es el estimador de máxima verosimilitud (MLE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué pasa si queremos proponer una distribución a priori del parámetro p, por ejemplo, la distribución uniforme?\n",
    "\n",
    "Entonces podemos utilizar la regla de bayes y maximizar no el Likelihood, sino la distribución a posteriori:\n",
    "\n",
    "$$ P(p | \\#1 = k)=\\frac{P(\\# 1=k|p) P(p)}{P(D)}$$\n",
    "\n",
    "Si bien $P(D)$ es una integral, no deja de ser un factor de normalización. Hagamos un ejemplo en el pizarrón."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo que la distribución a priori $P(p)$ funciona como una distribución de partida, y a medida que vamos ganando evidencia en favor de otro valor de p, la influencia de la distribución a priori es cada vez menor. Se puede observar que la distribución final a medida que la evidencia aumenta, es independiente de la distribución a priori.\n",
    "\n",
    "Para poca evidencia, es decir, pocas observaciones, la influencia de la evidencia es mayor y si bien eso aumenta el sesgo, baja la varianza del estimador. Si la distribución a priori está bien elegida. Esto puede ser una ventaja."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
